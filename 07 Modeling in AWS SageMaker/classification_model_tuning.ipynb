{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979e152f",
   "metadata": {},
   "source": [
    "<h3>Import and Process Data</h3>\n",
    "\n",
    "In this example, we use the Breast Cancer data\n",
    "\n",
    "Again, this data is very clean and thus does not need any preprocessing besides train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a231bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((559, 9), (140, 9), (559,), (140,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bc = pd.read_csv('breast-cancer.csv')\n",
    "bc.head()\n",
    "\n",
    "features = bc.drop(['ID','Class'], axis=1).values\n",
    "label = bc['Class'].values\n",
    "\n",
    "trainX_prc, testX_prc, trainY, testY = train_test_split(features,label,test_size=0.2)\n",
    "trainX_prc.shape, testX_prc.shape, trainY.shape, testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f445fb",
   "metadata": {},
   "source": [
    "<h3> Model Tuning with Grid Search and Cross Validation </h3>\n",
    "\n",
    "Tuning models in sklearn is straight-forward. In general, you need to\n",
    "1. Set up a parameter grid\n",
    "2. Create an empty model\n",
    "3. Create a GridSearchCV object using the created model and the parameter grid. Also remember to set the cv fold and the scoring here (accuracy for classification and r2_score for regression)\n",
    "4. Train the GridSearchCV object using fit()\n",
    "\n",
    "<h4> Logistic Regression </h4>\n",
    "\n",
    "We only need to tune <b>C</b> for logistic regression. \n",
    "\n",
    "\\<Test the different between keeping and dropping ID\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e657449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=5000),\n",
       "             param_grid=[{'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10,\n",
       "                                50, 100]}],\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "#hyperparameter grid for logistic regression, we only optimize regularization term C\n",
    "param_grid = [{'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1 , 5, 10, 50, 100]}]\n",
    "\n",
    "#create new model\n",
    "logistic = LogisticRegression(max_iter=5000)\n",
    "\n",
    "#perform grid search with 5-fold cross validation\n",
    "grid_search = GridSearchCV(logistic, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "grid_search.fit(trainX_prc,trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4209b1b",
   "metadata": {},
   "source": [
    "Let's look at the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b858b526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.05}\n",
      "0.9624195624195625\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b18b8",
   "metadata": {},
   "source": [
    "And apply it on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "038e0264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression accuracy:  0.9785714285714285\n",
      "logistic regression f1:  0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "best_logistic = grid_search.best_estimator_\n",
    "testY_pred_logistic = best_logistic.predict(testX_prc)\n",
    "print('logistic regression accuracy: ', accuracy_score(testY, testY_pred_logistic))\n",
    "print('logistic regression f1: ', f1_score(testY, testY_pred_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc3e7e1",
   "metadata": {},
   "source": [
    "<h4> Support Vector Machine </h4>\n",
    "\n",
    "We need to tune C, kernel (poly or rbf), coef0 and degree for poly kernel, and gamma for rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "441cfe98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid=[{'C': [0.01, 0.1, 1, 10, 100],\n",
       "                          'gamma': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
       "                          'kernel': ['rbf']}],\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "param_grid = [{\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'kernel' : ['rbf'],\n",
    "    'gamma' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "grid_search.fit(trainX_prc,trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2380603",
   "metadata": {},
   "source": [
    "Let's check the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f6c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.9677767052767052\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a2c75",
   "metadata": {},
   "source": [
    "And test it on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d6ee68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support vector machine accuracy:  0.9642857142857143\n",
      "support vector machine f1:  0.9438202247191012\n"
     ]
    }
   ],
   "source": [
    "best_svc = grid_search.best_estimator_\n",
    "testY_pred_svc = best_svc.predict(testX_prc)\n",
    "print('support vector machine accuracy: ', accuracy_score(testY, testY_pred_svc))\n",
    "print('support vector machine f1: ', f1_score(testY, testY_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd9134",
   "metadata": {},
   "source": [
    "<h4> Decision Tree </h4>\n",
    "\n",
    "We need to tune max_depth, max_features, min_sample_split, and min_sample_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d68b85d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(),\n",
       "             param_grid=[{'max_depth': [3, 4, 5, 6, 7],\n",
       "                          'max_features': [3, 5, 7],\n",
       "                          'min_samples_leaf': [10, 20, 30, 40],\n",
       "                          'min_samples_split': [10, 20, 30, 40]}],\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid = [{\n",
    "    'max_depth': [3,4,5,6,7],\n",
    "    'max_features' : [3, 5, 7],\n",
    "    'min_samples_split' : [10, 20, 30, 40],\n",
    "    'min_samples_leaf' : [10, 20, 30, 40]\n",
    "}]\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "grid_search.fit(trainX_prc,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f04a3dc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 7, 'max_features': 3, 'min_samples_leaf': 20, 'min_samples_split': 40}\n",
      "0.9499034749034749\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e4b5515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree accuracy:  0.9\n",
      "decision tree f1:  0.8541666666666667\n"
     ]
    }
   ],
   "source": [
    "best_dt = grid_search.best_estimator_\n",
    "testY_pred_dt = best_dt.predict(testX_prc)\n",
    "print('decision tree accuracy: ', accuracy_score(testY, testY_pred_dt))\n",
    "print('decision tree f1: ', f1_score(testY, testY_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a076f",
   "metadata": {},
   "source": [
    "<h4>Random Forest</h4>\n",
    "\n",
    "Similar to trees but with n_estimators added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcc8ca7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid=[{'max_depth': [3, 4, 5, 6], 'max_features': [3, 5, 7],\n",
       "                          'min_samples_leaf': [10, 20, 30, 40],\n",
       "                          'min_samples_split': [10, 20, 30, 40],\n",
       "                          'n_estimators': [5, 10, 20, 50]}],\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = [{\n",
    "    'n_estimators' : [5, 10, 20, 50],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'max_features' : [3, 5, 7],\n",
    "    'min_samples_split' : [10, 20, 30, 40],\n",
    "    'min_samples_leaf' : [10, 20, 30, 40]\n",
    "}]\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "grid_search.fit(trainX_prc,trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63020ac",
   "metadata": {},
   "source": [
    "Best training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87af0ae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6, 'max_features': 3, 'min_samples_leaf': 20, 'min_samples_split': 30, 'n_estimators': 10}\n",
      "0.9696106821106822\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27165e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest accuracy:  0.9714285714285714\n",
      "random forest f1:  0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "best_rf = grid_search.best_estimator_\n",
    "testY_pred_rf = best_rf.predict(testX_prc)\n",
    "print('random forest accuracy: ', accuracy_score(testY, testY_pred_rf))\n",
    "print('random forest f1: ', f1_score(testY, testY_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95951fa3",
   "metadata": {},
   "source": [
    "<h3> Model Tuning for AWS XGBoost </h3>\n",
    "\n",
    "<h4> Setting up Environment </h4>\n",
    "\n",
    "Since we are working with the external platfrom SageMaker, we need to perform a few steps of environment setting up. This requires importing and using sagemaker and boto3 to manage the sagemaker sessions.\n",
    "\n",
    "The code below create a new sagemaker session and set up storage location, and does not require modifications in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "524e947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()                    # Set a default S3 bucket for storing training, validation, and testing data\n",
    "prefix = 'breast-cancer'                          # the folder to store your data in the S3 instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c27f73d",
   "metadata": {},
   "source": [
    "<h4>Preparing Data</h4>\n",
    "\n",
    "We cannot use sklearn GridsearchCV to tune AWS models since they are not sklearn models. Instead, we utilize the tuning job application in SageMaker, which requires manually splitting training data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02491dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#further split training data to training and validation\n",
    "trainX_prc, validX_prc, trainY, validY = train_test_split(trainX_prc,trainY,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4dc10c",
   "metadata": {},
   "source": [
    "Input data to SageMaker models needs to be a single dataframe with the first column being the target. We will reorganize the training and validation data that way and upload it to the session's bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be8532f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorganize the data in format <label - features>\n",
    "traindata = np.concatenate([trainY.reshape(-1,1),trainX_prc],axis=1)\n",
    "validdata = np.concatenate([validY.reshape(-1,1),validX_prc],axis=1)\n",
    "\n",
    "#generate csv files to upload\n",
    "pd.DataFrame(traindata).to_csv('train.csv', index=False, header=False)\n",
    "pd.DataFrame(validdata).to_csv('validation.csv', index=False, header=False)\n",
    "\n",
    "#upload training and validation data to the s3 bucket\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf796f",
   "metadata": {},
   "source": [
    "<h4>XGBoost Model</h4>\n",
    "\n",
    "<h5>Define Hyperparamter Grid and Training Job</h5>\n",
    "\n",
    "Creating a parameter grid in SageMaker is slightly complicated than sklearn, however, is in the same general direction. We create a dictionary that consists of hyperparameters and their ranges. \n",
    "\n",
    "For simplicity, the part where the hyperparameters are defined is extracted into sagemaker_models.py, so please go there if you want to view or edit (optional). Otherwise, we will import everything from the module.\n",
    "\n",
    "We just need to create a hyperparameter dictionary similarly to in sklearn, then call the prewritten function to train the tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab48d31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................................!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker_models import *\n",
    "\n",
    "from sagemaker.parameter import CategoricalParameter, ContinuousParameter, IntegerParameter\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'eta': ContinuousParameter(0, 1), \n",
    "    'min_child_weight': ContinuousParameter(1, 10),\n",
    "    'alpha': ContinuousParameter(0, 2), \n",
    "    'max_depth': IntegerParameter(1, 10)\n",
    "}\n",
    "\n",
    "xgb_tuner = get_xgb_classifier(region, bucket, prefix, sess, role, hyperparameter_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c320b06a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-02-27 19:18:07 Starting - Found matching resource for reuse\n",
      "2023-02-27 19:18:07 Downloading - Downloading input data\n",
      "2023-02-27 19:18:07 Training - Training image download completed. Training in progress.\n",
      "2023-02-27 19:18:07 Uploading - Uploading generated training model\n",
      "2023-02-27 19:18:07 Completed - Resource reused by training job: sagemaker-xgboost-230227-1911-014-31739c71\n",
      "-------!"
     ]
    }
   ],
   "source": [
    "best_xgboost = xgb_tuner.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', model_name='xgboost-cls')\n",
    "\n",
    "testY_pred_xgb = predict_xgb_cls(best_xgboost, testX_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3aefd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost accuracy:  0.9714285714285714\n",
      "xgboost f1:  0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "print('xgboost accuracy: ', accuracy_score(testY, testY_pred_xgb))\n",
    "print('xgboost f1: ', f1_score(testY, testY_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ccb8f",
   "metadata": {},
   "source": [
    "<h4>Removing XGBoost Endpoint</h4>\n",
    "\n",
    "In practice, we may want to keep the endpoint and model to use later on. However, the free tier account we use limits having only one endpoint at a time, so we need to remove xgboost endpoint before moving on to linear learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2348ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgboost.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2602f0da",
   "metadata": {},
   "source": [
    "<h3>Linear Learner</h3>\n",
    "\n",
    "Very similarly to XGBoost, we need to define the hyperparameter dictionary then call the tuner function for classification linear learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d536929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.0-1.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"l1\": ContinuousParameter(1e-7, 1, scaling_type=\"Auto\"),\n",
    "    \"wd\": ContinuousParameter(1e-7, 1, scaling_type=\"Auto\"),\n",
    "    \"learning_rate\": ContinuousParameter(1e-5, 1, scaling_type=\"Auto\"),\n",
    "    \"mini_batch_size\": IntegerParameter(100, 300, scaling_type=\"Auto\"),\n",
    "}\n",
    "\n",
    "ll_tuner = get_ll_classifier(region, bucket, prefix, sess, role, hyperparameter_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b336584c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-02-27 19:30:23 Starting - Found matching resource for reuse\n",
      "2023-02-27 19:30:23 Downloading - Downloading input data\n",
      "2023-02-27 19:30:23 Training - Training image download completed. Training in progress.\n",
      "2023-02-27 19:30:23 Uploading - Uploading generated training model\n",
      "2023-02-27 19:30:23 Completed - Resource retained for reuse\n",
      "---------!"
     ]
    }
   ],
   "source": [
    "best_ll = ll_tuner.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', model_name='ll-cls')\n",
    "\n",
    "testY_pred_ll = predict_ll_cls(best_ll, testX_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "791f406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear learner accuracy:  0.9642857142857143\n",
      "linear learner f1:  0.9411764705882352\n"
     ]
    }
   ],
   "source": [
    "print('linear learner accuracy: ', accuracy_score(testY, testY_pred_ll))\n",
    "print('linear learner f1: ', f1_score(testY, testY_pred_ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957ec68",
   "metadata": {},
   "source": [
    "<h2>Cleaning up</h2>\n",
    "\n",
    "Please always run these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26eb1052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': 'JVKT5JFYRWK4RMFC',\n",
       "   'HostId': 'Vm8vkBZl7fNBZtvzpnV4redluSKiyidoFMbJ4S0l/ohkgkwL3FWdu2wJ93ntmBiw2Iwg6SI3NkY=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': 'Vm8vkBZl7fNBZtvzpnV4redluSKiyidoFMbJ4S0l/ohkgkwL3FWdu2wJ93ntmBiw2Iwg6SI3NkY=',\n",
       "    'x-amz-request-id': 'JVKT5JFYRWK4RMFC',\n",
       "    'date': 'Mon, 27 Feb 2023 19:35:25 GMT',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3',\n",
       "    'connection': 'close'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'breast-cancer/output/linear-learner-230227-1922-008-0217bcae/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-009-07a3f06e/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-003-ee09770b/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-001-b20681be/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-005-71d854ab/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-002-e0cd75ff/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-007-6824f9c0/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-010-36f5bad5/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-004-2d4f26f1/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-015-cb3c8c05/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-001-a571a77d/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-010-1f9e155e/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-012-6b12614a/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-006-4dbe8ede/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-008-46734f3e/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-014-31739c71/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/train/train.csv'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-004-5cfe42d4/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-009-bd58bc85/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-014-d204e4bc/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-011-a18003e6/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-015-a239e681/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-002-586460d2/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-007-200da06c/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-011-a214ac5e/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-012-160ff5ac/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/validation/validation.csv'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-006-175a48c4/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/sagemaker-xgboost-230227-1911-013-3952de3d/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-005-b6bd377a/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-013-1023c314/output/model.tar.gz'},\n",
       "   {'Key': 'breast-cancer/output/linear-learner-230227-1922-003-6e070ed0/output/model.tar.gz'}]}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete linear learner endpoint\n",
    "best_ll.delete_endpoint(delete_endpoint_config=True)\n",
    "\n",
    "#create session client to further clean up\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# delete models\n",
    "sagemaker_client.delete_model(ModelName='xgboost-cls')\n",
    "sagemaker_client.delete_model(ModelName='ll-cls')\n",
    "\n",
    "# delete bucket\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
